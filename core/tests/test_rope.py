import unittest

import torch

from core.models.embedding import RotaryEmbedding, apply_rotary_emb

# Generated by CodiumAI

from core.models.embedding import RotaryEmbedding

import unittest


class TestRotaryEmbedding(unittest.TestCase):

    def setUp(self):
        self.embed_dim = 64
        self.max_positions = 2048
        self.batch_size = 2
        self.seq_len = 20
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.rotary_embedding = RotaryEmbedding(self.embed_dim, self.max_positions).to(self.device)

    #  Initialization with default base value
    def test_initialization_with_default_base_value(self):
        embed_dim = 64
        max_positions = 2048
        rotary_embedding = RotaryEmbedding(embed_dim, max_positions)
        self.assertEqual(rotary_embedding.base, 10000)
        self.assertEqual(rotary_embedding.embed_dim, embed_dim)
        self.assertEqual(rotary_embedding.max_positions, max_positions)

    #  Initialization with custom base value
    def test_initialization_with_custom_base_value(self):
        embed_dim = 64
        max_positions = 2048
        custom_base = 5000
        rotary_embedding = RotaryEmbedding(embed_dim, max_positions, base=custom_base)
        self.assertEqual(rotary_embedding.base, custom_base)
        self.assertEqual(rotary_embedding.embed_dim, embed_dim)
        self.assertEqual(rotary_embedding.max_positions, max_positions)

    #  Forward pass with valid input tensors
    def test_forward_pass_with_valid_input_tensors(self):
        embed_dim = 64
        max_positions = 2048
        batch_size = 2
        seq_len = 20
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        rotary_embedding = RotaryEmbedding(embed_dim, max_positions).to(device)
        xq = torch.randn(batch_size, seq_len, embed_dim).to(device)
        xk = torch.randn(batch_size, seq_len, embed_dim).to(device)
        xq_out, xk_out = rotary_embedding(xq, xk, start=0)
        self.assertEqual(xq_out.shape, xq.shape)
        self.assertEqual(xk_out.shape, xk.shape)

    #  Initialization with embed_dim not divisible by 2
    def test_initialization_with_invalid_embed_dim(self):
        embed_dim = 63  # Not divisible by 2
        max_positions = 2048
        with self.assertRaises(ValueError):
            RotaryEmbedding(embed_dim, max_positions)

    #  Forward pass with mismatched dimensions between xq and xk
    def test_forward_pass_with_mismatched_dimensions(self):
        embed_dim = 64
        max_positions = 2048
        batch_size = 2
        seq_len = 20
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        rotary_embedding = RotaryEmbedding(embed_dim, max_positions).to(device)
        xq = torch.randn(batch_size, seq_len, embed_dim).to(device)
        xk = torch.randn(batch_size, seq_len + 1, embed_dim).to(device)  # Mismatched dimension
        with self.assertRaises(RuntimeError):
            rotary_embedding(xq, xk, start=0)

    def test_freqs_cis_precomputation(self):
        freqs_cis = self.rotary_embedding.get_freqs_cis(0, self.seq_len)
        self.assertEqual(freqs_cis.shape, (self.seq_len, self.embed_dim // 2, 2))

    def test_forward_pass_exceeding_max_positions(self):
        embed_dim = 64
        max_positions = 10000
        batch_size = 2
        seq_len = max_positions + 1
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        rotary_embedding = RotaryEmbedding(embed_dim, max_positions).to(device)
        xq = torch.randn(batch_size, seq_len, embed_dim).to(device)
        xk = torch.randn(batch_size, seq_len, embed_dim).to(device)
        with self.assertRaises(AssertionError):
            rotary_embedding(xq, xk, start=0)

    def test_computation_of_rotary_embeddings_using_apply_rotary_emb_function(self):
        xq = torch.randn(self.batch_size, self.seq_len, self.embed_dim).to(self.device)
        xk = torch.randn(self.batch_size, self.seq_len, self.embed_dim).to(self.device)
        start = 0
        xq_out, xk_out = self.rotary_embedding(xq, xk, start)
        expected_xq_out, expected_xk_out = apply_rotary_emb(xq, xk,
                                                            self.rotary_embedding.freqs_cis[start:start + self.seq_len])
        self.assertTrue(torch.allclose(xq_out, expected_xq_out))
        self.assertTrue(torch.allclose(xk_out, expected_xk_out))

    def test_retrieval_of_precomputed_frequency_tensor_within_valid_range(self):
        start = 0
        end = 10
        freqs_cis = self.rotary_embedding.get_freqs_cis(start, end)
        self.assertEqual(freqs_cis.shape, (end - start, self.embed_dim // 2, 2))


if __name__ == '__main__':
    unittest.main()
